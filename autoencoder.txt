import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc

# For reproducibility
np.random.seed(42)

data = pd.read_csv("creditcard.csv")

print("\nMissing values in dataset:", data.isnull().sum().sum())

# Separate features and target
X = data.drop(['Class'], axis=1)
y = data['Class']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Use only normal (non-fraud) data for training
X_train_normal = X_train[y_train == 0]
X_test_normal = X_test[y_test == 0] # 0-normal, 1-fraud
X_test_fraud = X_test[y_test == 1]

print(f"\nTraining normal samples: {X_train_normal.shape[0]}")
print(f"Testing normal samples: {X_test_normal.shape[0]}")
print(f"Testing fraud samples: {X_test_fraud.shape[0]}")

input_dim = X_train_normal.shape[1]   # Number of features
encoding_dim = 14                     # Latent dimension (bottleneck)

# Encoder layers
input_layer = Input(shape=(input_dim,))
encoder = Dense(encoding_dim * 2, activation='relu')(input_layer)
encoder = Dense(encoding_dim, activation='relu')(encoder)

decoder = Dense(encoding_dim * 2, activation='relu')(encoder)
decoder = Dense(input_dim, activation='sigmoid')(decoder)

# Combine encoder and decoder into autoencoder model
autoencoder = Model(inputs=input_layer, outputs=decoder)

autoencoder.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse',      # Mean Squared Error (reconstruction loss)
    metrics=['mae']  # Mean Absolute Error for monitoring
)

autoencoder.summary()

# Train the Autoencoder

history = autoencoder.fit(
    X_train_normal, X_train_normal,    # Input = Output
    epochs=15,
    batch_size=128,
    validation_data=(X_test_normal, X_test_normal),
    verbose=1
)

# Evaluate Reconstruction Error

reconstructions = autoencoder.predict(X_test)
mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)

error_df = pd.DataFrame({'Reconstruction_error': mse, 'True_class': y_test.values})

plt.figure(figsize=(8, 6))
plt.hist(error_df[error_df.True_class == 0].Reconstruction_error, bins=50, alpha=0.6, label="Normal")
plt.hist(error_df[error_df.True_class == 1].Reconstruction_error, bins=50, alpha=0.6, label="Fraud")
plt.title("Reconstruction Error Distribution")
plt.xlabel("Reconstruction error")
plt.ylabel("Count")
plt.legend()
plt.show()

# Determine threshold for anomaly detection

threshold = np.percentile(error_df.Reconstruction_error, 95)
print(f"Chosen threshold (95th percentile): {threshold:.6f}")

error_df['Predicted'] = (error_df.Reconstruction_error > threshold).astype(int)

print("\nConfusion Matrix:")
print(confusion_matrix(error_df.True_class, error_df.Predicted))

print("\nClassification Report:")
print(classification_report(error_df.True_class, error_df.Predicted))
